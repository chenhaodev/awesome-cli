#!/usr/bin/env python

'''
pip install argparse requests langchain-community langchain
'''

import argparse
import base64
import requests
import json
import subprocess
from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import FastEmbedEmbeddings
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import PromptTemplate
from langchain.vectorstores.utils import filter_complex_metadata

class ChatPDF:
    def __init__(self):
        self.vector_store = None
        self.retriever = None
        self.chain = None
        self.model = ChatOllama(model="mistral")
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
        self.prompt = PromptTemplate.from_template("""
            <s> [Instruction] You are an assistant tasked with answering questions based on the provided document. Utilize the context from the document to formulate your response. If the answer is not available within the document, indicate that the information is not available. Aim for responses that are direct, informative, and no longer than three sentences. [/Instruction] </s>
            [Instruction] Question: {question}
            Context: {context}
            Answer: [/Instruction]
        """)

    def ingest(self, file_path: str):
        file_type = file_path.split('.')[-1].lower()
        try:
            if file_type == 'pdf':
                docs = PyPDFLoader(file_path=file_path).load()
            elif file_type == 'html':
                docs = UnstructuredHTMLLoader(file_path=file_path).load()
            elif file_type == 'txt':
                docs = TextLoader(file_path=file_path).load()
            else:
                return "Unsupported file type"
        except Exception as e:
            return f"Failed to load document: {str(e)}"

        chunks = self.text_splitter.split_documents(docs)
        chunks = filter_complex_metadata(chunks)

        self.vector_store = Chroma.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 3,
                "score_threshold": 0.2,
            },
        )

        self.chain = ({"context": self.retriever, "question": RunnablePassthrough()}
                      | self.prompt
                      | self.model
                      | StrOutputParser())

    def ask(self, query: str):
        if not self.chain:
            return "Please, add a document first."

        try:
            return self.chain.invoke(query)
        except Exception as e:
            return f"Error during query processing: {str(e)}"

    def clear(self):
        self.vector_store = None
        self.retriever = None
        self.chain = None

class ChatMode:
    def __init__(self, model='mistral'):
        self.model = model

    def get_response(self, question):
        """Send request to the Ollama API with the question and print the response."""
        payload = {
            "model": self.model,
            "prompt": question,
            "stream": False,
        }
        api_url = 'http://localhost:11434/api/generate'
        response = requests.post(api_url, json=payload)
        if response.status_code == 200:
            json_response = response.json()
            print("Response from the model:", json_response.get("response"))
        else:
            print("Failed to get a response from the model, status code:", response.status_code)

    def interactive_mode(self):
        """Handle interactive mode where the user can input multiple questions."""
        print("Entering interactive mode. Type 'exit' to quit.")
        while True:
            question = input("Enter your question: ")
            if question.lower() == 'exit':
                break
            else:
                self.get_response(question=question)

    def quick_mode(self, question):
        """Handle quick mode where the user can input a single question."""
        print("Quick mode: processing your question.")
        self.get_response(question=question)

    def ollama_mode(self):
        """Start chat mode using `ollama run mistral`."""
        print("Starting Ollama Mistral in chat mode...")
        try:
            subprocess.run(["ollama", "run", "mistral"], check=True)
        except subprocess.CalledProcessError as e:
            print(f"Failed to start Ollama Mistral in chat mode: {e}")

class ChatImage:
    def __init__(self, model='llava:13b'):
        self.model = model
        self.base64_image_string = None

    def encode_image_to_base64(self, image_path):
        """Encode the image to a base64-encoded string."""
        with open(image_path, "rb") as image_file:
            self.base64_image_string = base64.b64encode(image_file.read()).decode('utf-8')

    def get_response(self, question):
        """Send request to the API and print the response."""
        if not self.base64_image_string:
            print("Please provide an image using the 'encode_image_to_base64' method before asking a question.")
            return

        payload = {
            "model": self.model,
            "prompt": question,
            "stream": False,
            "images": [self.base64_image_string]
        }
        api_url = 'http://localhost:11434/api/generate'
        response = requests.post(api_url, json=payload)
        if response.status_code == 200:
            json_response = response.json()
            print("Response from the model:", json_response.get("response"))
        else:
            print(f"Failed to get a response from the model, status code: {response.status_code}", file=sys.stderr)

    def interactive_mode(self):
        """Handle interactive mode where the user can input multiple questions."""
        print("Entering interactive mode. Type 'exit' to quit.")
        while True:
            question = input("Enter your question: ")
            if question.lower() == 'exit':
                break
            else:
                self.get_response(question)

def main():
    parser = argparse.ArgumentParser(description='Interact with the Ollama API, ask questions about a document or an image.')
    parser.add_argument('-f', '--file', help='Path to the file (e.g. pdf, html, txt, jpg, png)')
    parser.add_argument('-t', '--text', help='Text input for the question or prompt')
    args = parser.parse_args()

    if args.file:
        file_extension = args.file.split('.')[-1].lower()
        if file_extension in ['txt', 'html', 'pdf']:
            chat_document = ChatPDF()
            chat_document.ingest(args.file)

            if args.text:
                answer = chat_document.ask(args.text)
                print(f"Answer: {answer}")
            else:
                print("Document mode: type 'exit' to quit.")
                while True:
                    question = input("Ask a question about the document: ")
                    if question.lower() == 'exit':
                        break
                    answer = chat_document.ask(question)
                    print(f"Answer: {answer}")

            chat_document.clear()
        elif file_extension in ['jpg', 'png']:
            chat_image = ChatImage()
            chat_image.encode_image_to_base64(args.file)

            if args.text:
                # Single question mode
                chat_image.get_response(args.text)
            else:
                # Interactive mode
                chat_image.interactive_mode()
        else:
            print("Unsupported file type. Please provide a file with a supported extension (txt, html, pdf, jpg, png).")
    else:
        chat_mode = ChatMode()

        if args.text:
            chat_mode.quick_mode(args.text)
        else:
            chat_mode.ollama_mode()

if __name__ == "__main__":
    main()
